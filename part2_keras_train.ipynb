{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDate : 2019, APRIL\\nAuthor : Apiwit Theeraporn\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Date : 2019, APRIL\n",
    "Author : Apiwit Theeraporn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# use tensorflow backend\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import PIL\n",
    "\n",
    "import cv2\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display,Image,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-deterministic input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'processed_images/'\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 215, 240\n",
    "\n",
    "# Batch_size to train\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train\n",
    "nb_epoch = 20  \n",
    "\n",
    "# Total number of convolutional filters to use ~ batch\n",
    "nb_filters = 8\n",
    "\n",
    "# Max pooling\n",
    "nb_pool = 2\n",
    "\n",
    "# Size of convolution kernel\n",
    "nb_conv = 5\n",
    "\n",
    "kernel_size = (nb_conv,nb_conv);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-deterministic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display : Use this method instead of cv2.imageShow\n",
    "def iPythonDisplay(img_arr, fmt='jpeg'):\n",
    "    with BytesIO() as output:\n",
    "        with PIL.Image.fromarray(img_arr) as img:\n",
    "            img.save(output,format=fmt);\n",
    "        img_byte = output.getvalue()\n",
    "    display(Image(data=img_byte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras load images from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 images belonging to 4 classes.\n",
      "Found 8 images belonging to 4 classes.\n",
      "all classes :  {'buff': 0, 'fist': 1, 'luv': 2, 'rock': 3}\n",
      "got total class = 4\n"
     ]
    }
   ],
   "source": [
    "# flow_from_directory will first load data from it's config\n",
    "\"\"\"\n",
    "The function will run after the image is resized and augmented. \n",
    "The function should take one argument: an image (Numpy tensor with rank 3), \n",
    "and should output a Numpy tensor with the same shape.\n",
    "\"\"\"\n",
    "datagen = image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2 # reserve some data for validation\n",
    ")\n",
    "\n",
    "# number of channels\n",
    "# For 'grayscale' use 1 ,and for 'rgb' images use 3\n",
    "img_channels = 1\n",
    "\n",
    "# import first then transform\n",
    "processed_train_generator = datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    batch_size=batch_size,\n",
    "    target_size = (img_rows, img_cols),\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode = 'grayscale',\n",
    "    subset='training',\n",
    ")\n",
    "\n",
    "processed_valid_generator = datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    batch_size=batch_size,\n",
    "    target_size = (img_rows, img_cols),\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode = 'grayscale',\n",
    "    subset='validation',\n",
    ")\n",
    "\n",
    "\n",
    "nb_classes = len(processed_train_generator.class_indices)\n",
    "print('all classes : ',processed_valid_generator.class_indices)\n",
    "print('got total class =',len(processed_train_generator.class_indices));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/apiwitt/Desktop/Python/python3/python368/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/apiwitt/Desktop/python/python3/python368/code/git/keras/keras/backend/tensorflow_backend.py:3721: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 211, 236, 8)       208       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 211, 236, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 207, 232, 8)       1608      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 207, 232, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 103, 116, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 103, 116, 8)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 95584)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               12234880  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 12,237,212\n",
      "Trainable params: 12,237,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(\n",
    "    nb_filters, \n",
    "    kernel_size,\n",
    "    input_shape=(img_rows, img_cols,img_channels)))\n",
    "    # i.e. input_shape=(128, 128, 3) for 128x128 RGB pictures\n",
    "convout1 = Activation('relu')\n",
    "model.add(convout1)\n",
    "model.add(Conv2D(nb_filters, (nb_conv, nb_conv)))\n",
    "convout2 = Activation('relu')\n",
    "model.add(convout2)\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/apiwitt/Desktop/Python/python3/python368/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "1/8 [==>...........................] - ETA: 10s - loss: 1.3786 - acc: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Users/apiwitt/.pyenv/versions/3.6.8/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-026133584eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#CPU 4 cores,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         )\n",
      "\u001b[0;32m~/Desktop/python/python3/python368/code/git/keras/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/python3/python368/code/git/keras/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/python3/python368/code/git/keras/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/python3/python368/code/git/keras/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/python3/python368/code/git/keras/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2965\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2968\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python/python3/python368/code/git/keras/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2923\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2925\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2926\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Python/python3/python368/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=processed_train_generator.batch_size\n",
    "\n",
    "STAMP = 'best_gesture_iteration'\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "        processed_train_generator,\n",
    "        epochs=nb_epoch,\n",
    "        steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "        verbose=1,\n",
    "        validation_data=processed_valid_generator,\n",
    "        use_multiprocessing = True,\n",
    "        workers = 2, #CPU 4 cores,\n",
    "        callbacks= [early_stopping, model_checkpoint]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data ex. in sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "# from keras.models import load_model\n",
    "# model = load_model('best_gesture_iteration.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 370ms/step\n",
      "all classes :  {'buff': 0, 'fist': 1, 'luv': 2, 'rock': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict_generator(\n",
    "    processed_valid_generator, \n",
    "    steps=None, \n",
    "    callbacks=None, \n",
    "    workers=2, \n",
    "    use_multiprocessing=True, \n",
    "    verbose=1)\n",
    "\n",
    "print('all classes : ',processed_valid_generator.class_indices)\n",
    "np.round(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_path = 'raw_images/train/rock_33.png'\n",
    "test_image = cv2.imread(test_path)\n",
    "iPythonDisplay(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgb --> grayscale with shape\n",
    "def grayMorphGradient(image):\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB);\n",
    "    m = cv2.getStructuringElement(cv2.MORPH_CROSS, kernel_size); # Cross Morph\n",
    "    morph = cv2.morphologyEx(rgb, cv2.MORPH_GRADIENT, m);\n",
    "    _open = cv2.morphologyEx(morph, cv2.MORPH_OPEN, kernel_size) # Eliminate background noise\n",
    "    final = cv2.cvtColor(_open, cv2.COLOR_RGB2GRAY);\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = grayMorphGradient(test_image)\n",
    "iPythonDisplay(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape Conv2D input\n",
    "conv2D_input = np.array(test_image)\n",
    "# normalize by /255\n",
    "conv2D_input = conv2D_input/255\n",
    "# reshape to (batch, rows, cols, channels) \n",
    "# img_rows, img_cols = 215,240 \n",
    "# img_channels : grayscale = 1\n",
    "conv2D_input = conv2D_input.reshape(1,img_rows,img_cols,img_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_gesture_iteration.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all classes :  ['buff', 'fist', 'luv', 'rock']\n",
      "result = [[0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "result = np.round(model.predict(conv2D_input))\n",
    "nb_classes = len(processed_train_generator.class_indices)\n",
    "print('all classes : ',list(processed_valid_generator.class_indices))\n",
    "print('result =',result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
